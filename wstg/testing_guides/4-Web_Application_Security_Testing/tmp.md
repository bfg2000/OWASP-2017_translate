<b>1.1 Проведение поисковой разведки на предмет утечки информации</b><br>
<ins>Провести поисковую разведку на предмет утечки информации</ins><br>
Чтобы поисковые системы работали, компьютерные программы (или роботы) регулярно получают данные (сканирование миллиардов страниц в Интернете. Эти программы находят веб-контент и функции, переходя по ссылкам с других страниц или просматривая карты сайта. Если веб-сайт использует специальный файл с именем robots.txt для перечисления страниц, которые не должны получать поисковые системы, то перечисленные на нем страницы будут проигнорированы. Это базовый обзор - Google предлагает более подробное объяснение того, как поисковик работает.
Тестировщики могут использовать поисковые системы для разведки веб-сайтов и веб-приложений. Есть прямые и косвенные элементы для обнаружения и разведки поисковыми системами: прямые методы относятся к поиску индексов и связанного с ними контента из кешей, в то время как косвенные методы относятся к изучению конфиденциальной информации о дизайне и конфигурации путем поиска на форумах, группах новостей и веб-сайтах тендеров.
После завершения сканирования робот поисковой системы начинает индексирование веб-контента на основе тегов и связанных атрибутов, таких как <TITLE>, чтобы вернуть релевантные результаты поиска. Если файл robots.txt не обновляется в течение времени существования веб-сайта, а встроенные метатеги HTML, которые инструктируют роботов не индексировать контент, не используются, тогда индексы могут содержать веб-контент, не предназначенный для отображения владельцами. Владельцы веб-сайтов могут использовать ранее упомянутый robots.txt, метатеги HTML, аутентификацию и инструменты, предоставляемые поисковыми системами, для удаления такого содержания.

<ins>Задачи теста</ins>
Определите, какая конфиденциальная информация о конструкции и конфигурации приложения, системы или организации предоставляется:
напрямую (на сайте организации) или косвенно (через сторонние сервисы).
<ins>Как протестировать</ins>
Используйте поисковую систему для поиска потенциально конфиденциальной информации. Это может включать:
- сетевые схемы и конфигурации;
- заархивированные сообщения и электронные письма администраторов или других ключевых сотрудников;
- процедуры входа в систему и форматы имен пользователей;
- имена пользователей, пароли и закрытые ключи;
- сторонние файлы конфигурации или файлы конфигурации облачной службы;
- выявление содержания сообщения об ошибке
- разработка
- тестирование
- приемочное тестирование пользователей (UAT) и промежуточные версии сайтов.

<ins>Поисковые системы</ins><br>
Не ограничивайте тестирование только одной поисковой системой, так как разные поисковые системы могут давать разные результаты. Результаты поисковой системы могут различаться по-разному, в зависимости от того, когда система в последний раз сканировала контент, и от алгоритма, который система использует для определения релевантных страниц. Рассмотрите возможность использования следующих поисковых систем (перечисленных в алфавитном порядке):
- Baidu, самая популярная поисковая система Китая.
- Bing, поисковая система, принадлежащая и управляемая Microsoft, вторая по популярности в мире. Поддерживает расширенный поиск по ключевым словам.
- binsearch.info, поисковая машина для бинарных групп новостей Usenet.
- Common Crawl, «открытое хранилище данных веб-сканирования, доступное и доступное для анализа кем угодно».
- DuckDuckGo, поисковая система, ориентированная на конфиденциальность, которая собирает результаты из множества различных источников. Поддерживает синтаксис поиска.
- Google, который предлагает самую популярную в мире поисковую систему и использует систему ранжирования, чтобы попытаться выдать наиболее релевантные результаты. Поддерживает поисковые операторы.
- Internet Archive Wayback Machine, «создание цифровой библиотеки Интернет-сайтов и других культурных артефактов в цифровой форме».
- Startpage - поисковая система, которая использует результаты Google без сбора личной информации с помощью трекеров и журналов. Поддерживает поисковые операторы.
- Shodan, сервис для поиска подключенных к Интернету устройств и услуг. Варианты использования включают ограниченный бесплатный план, а также планы платной подписки.
И DuckDuckGo, и Startpage предлагают пользователям повышенную конфиденциальность за счет отказа от трекеров и ведения журналов. Это может уменьшить утечку информации о тестере.

<ins>Операторы поиска</ins>
Оператор поиска - это специальное ключевое слово или синтаксис, который расширяет возможности обычных поисковых запросов и может помочь получить более конкретные результаты. Обычно они имеют форму оператор: запрос. Вот несколько обычно поддерживаемых поисковых операторов:
- site: ограничит поиск предоставленным доменом.
- inurl: вернет только результаты, содержащие ключевое слово в URL.
- intitle: вернет только результаты, содержащие ключевое слово в заголовке страницы.
- intext: или inbody: будет искать только ключевое слово в теле страниц.
- filetype: будет соответствовать только определенному типу файла, например png или php.
Например, чтобы найти веб-контент owasp.org, проиндексированный типичной поисковой системой, требуется следующий синтаксис:<br>
site:owasp.org

Просмотр кэшированного содержимого
Для поиска содержимого, которое ранее было проиндексировано, используйте оператор cache :. Это полезно для просмотра содержимого, которое могло измениться с момента его индексации или быть недоступным. Не все поисковые системы предоставляют для поиска кэшированный контент; самый полезный источник на момент написания - Google.

Чтобы просмотреть файл owasp.org в кешированном виде, используйте следующий синтаксис:<br>
cache:owasp.org

<ins>Google Хакинг или Доркинг</ins><br>
Поиск с помощью операторов может быть очень эффективным методом обнаружения в сочетании с творческими способностями тестировщика. Операторы могут быть объединены в цепочку для эффективного обнаружения определенных типов конфиденциальных файлов и информации. Этот метод, называемый <a href="https://en.wikipedia.org/wiki/Google_hacking">Google hacking</a> или Dorking, также возможен с использованием других поисковых систем, если поддерживаются поисковые операторы.

База данных придурков, такая как <a href="https://www.exploit-db.com/google-hacking-database">Google Hacking Database</a>, является полезным ресурсом, который может помочь раскрыть конкретную информацию. Некоторые категории, доступные в этой базе данных, включают:
- Опоры
- Файлы, содержащие имена пользователей
- Секретные каталоги
- Обнаружение веб-сервера
- Уязвимые файлы
- Уязвимые серверы
- Сообщения об ошибках
- Файлы с конфиденциальной информацией
- Файлы, содержащие пароли
- Конфиденциальная информация о покупках в Интернете

Базы данных для других поисковых систем, таких как Bing и Shodan, доступны на таких ресурсах, как <br> 
<a href="https://resources.bishopfox.com/resources/tools/google-hacking-diggity/">Google Hacking Diggity Project от Bishop Fox.</a>

Дополнение:
Внимательно изучите конфиденциальность информации и конфигурации, прежде чем она будет размещена в Интернете.
Периодически проверяйте конфиденциальность существующей информации о конструкции и конфигурации, размещенной в Интернете.
 
 
<b>1.2 Отпечаток Веб-сервера</b>

Отпечатки веб-сервера - это задача определения типа и версии веб-сервера, на котором работает цель. Хотя идентификация веб-сервера часто инкапсулируется в средствах автоматического тестирования, исследователям важно понимать основы того, как эти инструменты пытаются идентифицировать программное обеспечение и почему это полезно.
Точное определение типа веб-сервера, на котором работает приложение, может позволить тестерам безопасности определить, уязвимо ли приложение для атаки. В частности, серверы с более старыми версиями программного обеспечения без актуальных исправлений безопасности могут быть уязвимы для известных эксплойтов, зависящих от версии.

<ins>Задачи теста</ins><br>
Определите версию и тип работающего веб-сервера, чтобы обеспечить дальнейшее обнаружение любых известных уязвимостей.<br>
<ins>Как протестировать</ins><br>
Методы, используемые для снятия отпечатков веб-сервера, включают захват баннеров, получение ответов на искаженные запросы и использование автоматизированных инструментов для выполнения более надежных сканирований, использующих комбинацию тактик. Фундаментальная предпосылка, по которой работают все эти методы, одинакова. Все они стремятся вызвать некоторый ответ от веб-сервера, который затем можно сравнить с базой данных известных ответов и поведения и, таким образом, сопоставить с известным типом сервера.

<ins>Захват баннера</ins><br>
Захват баннера выполняется путем отправки HTTP-запроса на веб-сервер и изучения его заголовка ответа. Это можно сделать с помощью различных инструментов, включая telnet для HTTP-запросов или openssl для запросов через SSL.

Например, вот ответ на запрос от сервера Apache:<br>
HTTP/1.1 200 OK<br>
Date: Thu, 05 Sep 2019 17:42:39 GMT<br>
Server: Apache/2.4.41 (Unix)<br>
Last-Modified: Thu, 05 Sep 2019 17:40:42 GMT<br>
ETag: "75-591d1d21b6167"<br>
Accept-Ranges: bytes<br>
Content-Length: 117<br>
Connection: close<br>
Content-Type: text/html<br>
...<br>


Вот еще один ответ, на этот раз от nginx:<br>
HTTP/1.1 200 OK<br>
Server: nginx/1.17.3<br>
Date: Thu, 05 Sep 2019 17:50:24 GMT<br>
Content-Type: text/html<br>
Content-Length: 117<br>
Last-Modified: Thu, 05 Sep 2019 17:40:42 GMT<br>
Connection: close<br>
ETag: "5d71489a-75"<br>
Accept-Ranges: bytes<br>
...<br>

Вот как выглядит ответ от lighttpd:<br>
HTTP/1.0 200 OK<br>
Content-Type: text/html<br>
Accept-Ranges: bytes<br>
ETag: "4192788355"<br>
Last-Modified: Thu, 05 Sep 2019 17:40:42 GMT<br>
Content-Length: 117<br>
Connection: close<br>
Date: Thu, 05 Sep 2019 17:57:57 GMT<br>
Server: lighttpd/1.4.54<br>

В этих примерах четко указаны тип и версия сервера. Однако приложения, заботящиеся о безопасности, могут скрыть информацию о своем сервере, изменив заголовок. <br>
Например, вот отрывок из ответа на запрос сайта с измененным заголовком:<br>
HTTP/1.1 200 OK<br>
Server: Website.com<br>
Date: Thu, 05 Sep 2019 17:57:06 GMT<br>
Content-Type: text/html; charset=utf-8<br>
Status: 200 OK<br>
...<br>

В случаях, когда информация о сервере скрыта, тестировщики могут угадать тип сервера на основе порядка полей заголовка.<br>
Обратите внимание, что в приведенном выше примере Apache поля следуют в следующем порядке:
- Дата
- Сервер
- Последнее изменение
- ETag
- Принять-диапазоны
- Content-Length
- Подключение
- Тип содержимого

Однако как в примерах nginx, так и в примерах скрытого сервера общие поля следуют в следующем порядке:
- Сервер
- Дата
- Тип содержимого
Тестировщики могут использовать эту информацию, чтобы предположить, что скрытый сервер - это nginx. Однако, учитывая, что ряд различных веб-серверов может использовать один и тот же порядок полей, и поля могут быть изменены или удалены, этот метод не является определенным.

<ins>Отправка неверных запросов</ins><br>
Веб-серверы можно идентифицировать, изучив их ответы на ошибки, а в тех случаях, когда они не были настроены, их страницы ошибок по умолчанию. Один из способов заставить сервер их представлять - это отправить намеренно неверные или искаженные запросы.

Например, вот ответ на запрос несуществующего метода SANTA CLAUS от сервера Apache.<br>
GET / SANTA CLAUS/1.1<br>

HTTP/1.1 400 Bad Request<br>
Date: Fri, 06 Sep 2019 19:21:01 GMT<br>
Server: Apache/2.4.41 (Unix)<br>
Content-Length: 226<br>
Connection: close<br>
Content-Type: text/html; charset=iso-8859-1<br>

<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN"><br>
<html><head><br>
<title>400 Bad Request</title><br>
</head><body><br>
<h3>Bad Request</h3><br>
<p>Your browser sent a request that this server could not understand.<br /><br>
</p><br>
</body></html><br>

Вот ответ на тот же запрос от nginx<br>
GET / SANTA CLAUS/1.1<br>

<html><br>
<head><title>404 Not Found</title></head><br>
<body><br>
<center><h3>404 Not Found</h3></center><br>
<hr><center>nginx/1.17.3</center><br>
</body><br>
</html><br>

Вот ответ на тот же запрос от lighttpd.<br>
GET / SANTA CLAUS/1.1<br>

HTTP/1.0 400 Bad Request<br>
Content-Type: text/html<br>
Content-Length: 345<br>
Connection: close<br>
Date: Sun, 08 Sep 2019 21:56:17 GMT<br>
Server: lighttpd/1.4.54<br>

<?xml version="1.0" encoding="iso-8859-1"?><br>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"<br>
         "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><br>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><br>
 <head><br>
  <title>400 Bad Request</title><br>
 </head><br>
 <body><br>
  <h3>400 Bad Request</h3><br>
 </body><br>
</html><br>

Поскольку страницы ошибок по умолчанию предлагают множество факторов, различающих типы веб-серверов, их изучение может быть эффективным методом для снятия отпечатков , даже если поля заголовка сервера скрыты.

<ins>Использование средств автоматического сканирования</ins><br>
Как указывалось ранее, снятие отпечатков веб-сервера часто включается в состав средств автоматического сканирования. Эти инструменты могут делать запросы, аналогичные показанным выше, а также отправлять другие, более специфичные для сервера запросы. Автоматизированные инструменты могут сравнивать ответы от веб-серверов намного быстрее, чем ручное тестирование, и использовать большие базы данных известных ответов для попытки идентификации сервера. По этим причинам автоматизированные инструменты с большей вероятностью дадут точные результаты.
Вот несколько часто используемых инструментов сканирования, которые включают функцию снятия отпечатков пальцев веб-сервера.
- Netcraft, онлайн-инструмент, который сканирует веб-сайты в поисках информации, включая веб-сервер.
- Nikto, инструмент сканирования командной строки с открытым исходным кодом.
- Nmap, инструмент командной строки с открытым исходным кодом, который также имеет графический интерфейс Zenmap.

<ins>Исправление</ins><br>
Хотя открытая информация о сервере не обязательно сама по себе является уязвимостью, это информация, которая может помочь злоумышленникам использовать другие уязвимости, которые могут существовать. Открытая информация о сервере также может побудить злоумышленников найти уязвимости сервера, зависящие от версии, которые могут быть использованы для эксплуатации не исправленных серверов. По этой причине рекомендуется принять некоторые меры предосторожности.<br>

Эти действия включают:
- Скрытие информации веб-сервера в заголовках, например, с помощью модуля Apache mod_headers.
- Использование усиленного обратного <a href="https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%BE%D0%BA%D1%81%D0%B8-%D1%81%D0%B5%D1%80%D0%B2%D0%B5%D1%80#Reverse_proxies">прокси-сервера</a> для создания дополнительного уровня безопасности между веб-сервером и Интернетом.
- Обеспечение актуальности веб-серверов с помощью последних версий программного обеспечения и исправлений безопасности.


<b>1.3 Проверка метафайлов веб-сервера на предмет утечки информации</b>

В этом разделе описывается, как протестировать различные файлы метаданных на предмет утечки информации о путях или функциональных возможностях веб-приложения. Кроме того, список каталогов, которые должны избегать пауки, роботы или обходчики, также может быть создан в качестве зависимости для путей выполнения карты через приложение. Другая информация 
также может быть собрана для определения поверхности атаки, деталей технологии или для использования в социальной инженерии.

<ins>Задачи теста</ins><br>
Выявление скрытых путей и функциональных возможностей посредством анализа файлов метаданных.
Извлеките и нанесите на карту другую информацию, которая может привести к лучшему пониманию имеющихся систем.

<ins>Как протестировать</ins><br>
Любое из действий, выполненных ниже с помощью wget, также можно выполнить с помощью curl. Многие инструменты динамического тестирования безопасности приложений (DAST), такие как ZAP и Burp Suite, включают в себя проверку или синтаксический анализ этих ресурсов как часть своих функций паука / сканера. Их также можно идентифицировать с помощью различных Google Dorks или функций расширенного поиска, таких как inurl :.

<ins>Роботы</ins><br>
Веб-пауки, роботы или сканеры извлекают веб-страницу, а затем рекурсивно просматривают гиперссылки для получения дополнительного веб-содержимого. Их допустимое поведение определяется протоколом исключения роботов файла robots.txt в корневом веб-каталоге.

В качестве примера ниже цитируется начало файла robots.txt от Google, отобранного 5 мая 2020 года:<br>
User-agent: *<br>
Disallow: /search<br>
Allow: /search/about<br>
Allow: /search/static<br>
Allow: /search/howsearchworks<br>
Disallow: /sdch<br>
...<br>

Директива User-Agent относится к конкретному веб-пауку / роботу / сканеру. 
Например, User-Agent: Googlebot относится к пауку от Google, а User-Agent: bingbot относится к сканеру от Microsoft. 
User-Agent: * в приведенном выше примере применяется ко всем веб-паукам / роботам / сканерам.

Директива Disallow указывает, какие ресурсы запрещены пауками / роботами / сканерами. В приведенном выше примере запрещены:<br>
...<br>
Disallow: /search<br>
...<br>
Disallow: /sdch<br>
...<br>

Веб-пауки / роботы / краулеры могут намеренно игнорировать директивы Disallow, указанные в файле robots.txt, например, из социальных сетей, чтобы гарантировать, что общие ссылки остаются действительными. Следовательно, robots.txt не следует рассматривать как механизм для обеспечения ограничений на доступ к веб-контенту, его хранение или повторную публикацию третьими сторонами.
Файл robots.txt извлекается из корневого веб-каталога веб-сервера. 

Например, чтобы получить файл robots.txt с www.google.com с помощью wget или curl:<br>
$ curl -O -Ss http://www.google.com/robots.txt && head -n5 robots.txt<br>
User-agent: *<br>
Disallow: /search<br>
Allow: /search/about<br>
Allow: /search/static<br>
Allow: /search/howsearchworks<br>
...<br>

Анализируйте robots.txt с помощью инструментов Google для веб-мастеров
Владельцы веб-сайтов могут использовать функцию Google «Анализировать robots.txt» для анализа веб-сайта как часть инструментов Google для веб-мастеров. Этот инструмент может помочь с тестированием, и процедура выглядит следующим образом:
- Войдите в Инструменты Google для веб-мастеров с учетной записью Google.
- На панели управления введите URL-адрес сайта, который нужно проанализировать.
- Выберите один из доступных методов и следуйте инструкциям на экране.

<ins>Мета-теги</ins>
Теги <META> расположены в разделе HEAD каждого HTML-документа и должны быть согласованы на всем веб-сайте в том случае, если начальная точка робота / паука / сканера не начинается с ссылки на документ, кроме корневого веб-сайта, то есть глубокой ссылки. Директива Robots также может быть указана с помощью специального тега META.

<ins>Мета-тег роботов</ins>
Если нет записи <META NAME = "ROBOTS" ...>, тогда "Протокол исключения роботов" по умолчанию имеет значение INDEX, FOLLOW соответственно. Таким образом, две другие допустимые записи, определенные протоколом исключения роботов, имеют префикс NO ... то есть NOINDEX и NOFOLLOW.
На основе директив Disallow, перечисленных в файле robots.txt в корневом каталоге веб-сайта, выполняется поиск по регулярному выражению для <META NAME = "ROBOTS" на каждой веб-странице, и результат сравнивается с файлом robots.txt в корневом веб-каталоге.

<ins>Разные мета-информационные теги</ins>
Организации часто встраивают информационные мета-теги в веб-контент для поддержки различных технологий, таких как программы чтения с экрана, предварительный просмотр социальных сетей, индексация поисковой системы и т. д. Такая метаинформация может быть полезна для тестировщиков при определении используемых технологий и дополнительных путей / функций для изучения и тестирования. <br>
Следующая метаинформация была получена с www.whitehouse.gov через View Page Source 5 мая 2020 г .:<br>
`
...
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="The White House" />
<meta property="og:description" content="We, the citizens of America, are now joined in a great national effort to rebuild our country and to restore its promise for all. – President Donald Trump." />
<meta property="og:url" content="https://www.whitehouse.gov/" />
<meta property="og:site_name" content="The White House" />
<meta property="fb:app_id" content="1790466490985150" />
<meta property="og:image" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta property="og:image:secure_url" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:description" content="We, the citizens of America, are now joined in a great national effort to rebuild our country and to restore its promise for all. – President Donald Trump." />
<meta name="twitter:title" content="The White House" />
<meta name="twitter:site" content="@whitehouse" />
<meta name="twitter:image" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta name="twitter:creator" content="@whitehouse" />
...
<meta name="apple-mobile-web-app-title" content="The White House">
<meta name="application-name" content="The White House">
<meta name="msapplication-TileColor" content="#0c2644">
<meta name="theme-color" content="#f5f5f5">
...
`
<ins>Файлы Sitemap</ins><br>
Карта сайта - это файл, в котором разработчик или организация могут предоставить информацию о страницах, видео и других файлах, предлагаемых сайтом или приложением, а также о взаимосвязи между ними. Поисковые системы могут использовать этот файл для более интеллектуального изучения вашего сайта. Тестировщики могут использовать файлы sitemap.xml, чтобы узнать больше о сайте или приложении, чтобы изучить его более полно.

Следующий отрывок взят из основной карты сайта Google, полученной 5 мая 2020 года.<br>
`$ wget --no-verbose https://www.google.com/sitemap.xml && head -n8 sitemap.xml
2020-05-05 12:23:30 URL:https://www.google.com/sitemap.xml [2049] -> "sitemap.xml" [1]`

`<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.google.com/schemas/sitemap/0.84">
  <sitemap>
    <loc>https://www.google.com/gmail/sitemap.xml</loc>
  </sitemap>
  <sitemap>
    <loc>https://www.google.com/forms/sitemaps.xml</loc>
  </sitemap>
...`
  
  Изучая, тестировщик может захотеть получить карту сайта gmail https://www.google.com/gmail/sitemap.xml:<br>
  
  `<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xhtml="http://www.w3.org/1999/xhtml">
<url>
    <loc>https://www.google.com/intl/am/gmail/about/</loc>
    <xhtml:link href="https://www.google.com/gmail/about/" hreflang="x-default" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/el/gmail/about/" hreflang="el" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/it/gmail/about/" hreflang="it" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/ar/gmail/about/" hreflang="ar" rel="alternate"/>
...`
    
   <ins>Безопасность TXT</ins><br>
security.txt - это предлагаемый стандарт, который позволяет веб-сайтам определять политики безопасности и контактные данные. Есть несколько причин, по которым это может быть интересно при тестировании сценариев, включая, помимо прочего:
- Определение дальнейших путей или ресурсов для включения в обнаружение / анализ.
- Сбор разведывательной информации с открытым исходным кодом.
- Поиск информации о Bug Bounties и т. д.
- Социальная инженерия.

Файл может находиться либо в корне веб-сервера, либо в каталоге .well-known /. Пример:
https://example.com/security.txt
https://example.com/.well-known/security.txt

Вот реальный пример, полученный в LinkedIn 2020 5 мая:<br>
`$ wget --no-verbose https://www.linkedin.com/.well-known/security.txt && cat security.txt
2020-05-07 12:56:51 URL:https://www.linkedin.com/.well-known/security.txt [333/333] -> "security.txt" [1]
# Conforms to IETF `draft-foudil-securitytxt-07`
Contact: mailto:security@linkedin.com
Contact: https://www.linkedin.com/help/linkedin/answer/62924
Encryption: https://www.linkedin.com/help/linkedin/answer/79676
Canonical: https://www.linkedin.com/.well-known/security.txt
Policy: https://www.linkedin.com/help/linkedin/answer/62924`

<ins>Люди TXT</ins><br>
human.txt - это инициатива по знакомству с людьми, стоящими за веб-сайтом. Он принимает форму текстового файла, который содержит информацию о разных людях, которые внесли свой вклад в создание веб-сайта. См. Humanstxt для получения дополнительной информации. Этот файл часто (хотя и не всегда) содержит информацию о вакансиях или сайтах / маршрутах.

Следующий пример был получен из Google 2020 5 мая:
`$ wget --no-verbose  https://www.google.com/humans.txt && cat humans.txt
2020-05-07 12:57:52 URL:https://www.google.com/humans.txt [286/286] -> "humans.txt" [1]
Google is built by a large team of engineers, designers, researchers, robots, and others in many different sites across the globe. It is updated continuously, and built with more tools and technologies than we can shake a stick at. If you'd like to help us out, see careers.google.com.`

<ins>Другие известные источники информации</ins><br>
Существуют и другие RFC и Интернет-проекты, предлагающие стандартизированное использование файлов в каталоге .well-known /. Списки которых можно найти <a href="https://en.wikipedia.org/wiki/List_of_/.well-known/_services_offered_by_webservers">здесь</a> или <a href="https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml">здесь</a>.
Для тестировщика было бы довольно просто просмотреть RFC / черновики, создать список, который будет предоставлен сканеру или фаззеру, чтобы проверить наличие или содержание таких файлов. 

<ins>TOOLS</ins>
- Browser (View Source or Dev Tools functionality)
- curl
- wget
- Burp Suite
- ZAP


<b>1.4 Перечисление приложений на веб-сервере</b>


<b>1.5 Проверка содержимого веб-страницы на предмет утечки информации</b>


<b>1.6 Определение точек входа в приложение</b>


<b>1.7 Сопоставление путей выполнения через приложение</b>


<b>1.8 Платформа веб-приложений отпечатков пальцев</b>


<b>1.9 Веб-приложение Fingerprint</b>


<b>1.10 Архитектура приложения карты</b>
